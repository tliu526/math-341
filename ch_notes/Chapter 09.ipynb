{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'.#Chapter 9: Tools (Expectation)\n",
    "\n",
    "##9.1 Calculus Motivation\n",
    "\n",
    "\n",
    "####Definition (Taylor Series)\n",
    "\n",
    "If $f$ is diferentiable $n$ times (with $f^{(k)}(x)$ denoting the $k^{th}$ derivative at $x$), its $n^{th}$ order Taylor series at the point $a$ is:\n",
    "\n",
    "$$T_n(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + ... + \\frac{f^{(n)}(a)}{n!}(x-a)^n = \\sum_{k=0}^n \\frac{f^{(k)} (a)}{k!}(x-a)^k$$\n",
    "\n",
    "We call $\\frac{f^{(k)} (a)}{k!}$ the $k^{th}$ Taylor coefficient of $f$ about $a$. Inmany applications we want the Taylor Series about the origin, so $a=0$. The Taylor series takes knowledge of the behavior of a function and its derivatives at a point, and uses that to estimate its values elsewhere.\n",
    "\n",
    "The more information we include, the better the approximation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.2 Expected Values and Moments\n",
    "\n",
    "####Definition (Expected Values, Moments)\n",
    "\n",
    "Let $X$ be a random variable on $\\mathbb{R}$ with probability density function $f_X$. The expected value of a function $g(X)$ is:\n",
    "\n",
    "$$\\mathbb{E}[g(X)] = \\begin{cases}\n",
    "    \\int_{-\\infty}^{\\infty} g(x) \\cdot f_X(dx)& \\text{if X is continuous}\\\\\n",
    "    \\sum_n g(x_n) \\cdot f_X(x_n)& \\text{if X is discrete}\n",
    "    \\end{cases}$$\n",
    "    \n",
    "The most important cases are when $g(x) = x^r$. We call $\\mathbb{E}[X^r]$ the $r^{th}$ moment of $X$, and $\\mathbb{E}[(X - \\mathbb{E}[X])^r]$ the $r^{th}$ centered moment of $X$.\n",
    "\n",
    "The __mean__ is the first moment.\n",
    "\n",
    "The __variance__ is the second centered moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.3 Mean and Variance\n",
    "\n",
    "####Definition (mean)\n",
    "\n",
    "The __mean__ of $X$ is the first moment. We denote it by $\\mathbb[X]$ or $\\mu_X$. Explicitly:\n",
    "\n",
    "$$\\mu = \\begin{cases}\n",
    "    \\int_{-\\infty}^{\\infty} x \\cdot f_X(x)dx & \\text{if X is continuous}\\\\\n",
    "    \\sum_n x_n \\cdot f_X(x_n) & \\text{if X is discrete}\n",
    "    \\end{cases}$$\n",
    "    \n",
    "####Definition (variance)\n",
    "\n",
    "The __variance__ of $X$, denoted $\\sigma_X^2$ or Var($X$), is the second centered moment, or equivalent the expected value of $g(X) = (X - \\mu_X)^2$; again, we often suppress the subscript $X$ if the random variable is clear, and write $\\sigma^2$. Writing it out in full:\n",
    "\n",
    "$$\\sigma_X^2 = \\begin{cases}\n",
    "    \\int_{-\\infty}^{\\infty} (x-\\mu_X)^2 f_X(x)dx & \\text{if X is continuous}\\\\\n",
    "    \\sum_n (x_n - \\mu_X)^2 f_X(x_n) & \\text{if X is discrete}\n",
    "    \\end{cases}$$\n",
    "    \n",
    "As $\\mu_X = \\mathbb{E}[X]$, after some algebra one finds:\n",
    "\n",
    "$$\\sigma^2 = \\mathbb{E}[(X-\\mathbb{E}[X]^2] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$$\n",
    "\n",
    "This relates the variance to the first two moments of $X$, and is usefl for many calculations. The __standard deviation__ is the square-root of the variance, or $\\sigma_X = \\sqrt{\\sigma_X^2}$.\n",
    "\n",
    "__Technical Note:__ In order for the mean to exist, we want $\\int_{-\\infty}^{\\infty} |x|f_x(x)dx$ or $\\sum_n |x_n|f_X(x_n)$ to be finite.\n",
    "\n",
    "The advantage the standard deviation has over the variance is that it has the same units as the mean. Thus, it's the standard deviation that provides the natural scale to view fluctuations about the mean.\n",
    "\n",
    "Like with Taylor coefficients, the more moments we know the more information we can gather about different PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.4 Joint Distributions\n",
    "\n",
    "####Definition (Joint Probability Density Function)\n",
    "\n",
    "Let $X_1, X_2,..., X_n$ be continuous random variables with densities $f_{X_1}, f_{X_2},...f_{X_n}$. Assume each $X_i$ is defined on a subset of $\\mathbb{R}$. The joint density function of the tuple $(X_1,...,x_n)$ is a non-negative, integrable function $f_{X_1},...,f_{X_n}$ such that, for every nice set $S \\subset \\mathbb{R}^n$ we have:\n",
    "\n",
    "- A lotta integrals on p. 281\n",
    "\n",
    "We call $f_{X_i}$ the __marginal__ density of X_i and obtain it by integrating out the other $n-1$ variables. The $n$ random variables are independent if and only if:\n",
    "\n",
    "$$f_{X_1},...,f_{X_n}(x_1,...,x_n) = f_{X_1}(x_1)...f_{X_n}(x_n).$$\n",
    "\n",
    "For discrete random variables, replace the integrals with sums. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.5 Linearity of Expectation\n",
    "\n",
    "####Theorem 9.5.1 (Linearity of Expectation)\n",
    "\n",
    "Let $X_1...X_n$ be random variables, let $g_1...g_n$ be functions such that $\\mathbb{E}[|g_i(X_i)|]$ exists and is finite, and let $a_1...a_n$ be any real numbers. Then:\n",
    "\n",
    "$$\\mathbb{E}[a_1g_1(X_1) + ... + a_n g_n(X_n)] = a_1 \\mathbb{E}[g_1(X_1)] + ... + a_n \\mathbb{E}[g_n(X_n))]$$\n",
    "\n",
    "Note the random variables are not assumed to be independent. Also, if $g_i(X_i) = c$ where $c$ is a constant, then $\\mathbb{E}[g_i(X_i)] = c$.\n",
    "\n",
    "####Lemma 9.5.2\n",
    "\n",
    "Let $X$ be a random variable with mean $\\mu_x$ and variance $\\sigma_X^2$. If $a$ and $b$ are any fixed constants, then for the random variable $Y= aX + b$ we have:\n",
    "\n",
    "- $\\mu_Y = a\\mu_X + b$\n",
    "- $\\sigma_Y^2 = a^2\\sigma_X^2$\n",
    "\n",
    "####Lemma 9.5.3\n",
    "\n",
    "Let $X$ be a random variable. Then:\n",
    "\n",
    "$$\\text{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.6 Properties of the Mean and the Variance\n",
    "\n",
    "####Theorem 9.6.1\n",
    "\n",
    "If $X$ and $Y$ are independent random variables, then:\n",
    "\n",
    "$$\\mathbb{E}[XY] = \\mathbb[X]\\mathbb[Y]$$\n",
    "\n",
    "A particularly important case is:\n",
    "\n",
    "$$\\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)] = \\mathbb{E}[X-\\mu_x]\\mathbb{E}[Y - \\mu_Y] = 0$$\n",
    "\n",
    "The joint density function of two independent random variables is the product of the marginal density functions.\n",
    "\n",
    "\n",
    "####Theorem 9.6.2 (Means and Variances of Sums of Random Variables)\n",
    "\n",
    "Let $X_1,...X_n$ be random variables. Then:\n",
    "\n",
    "$$\\mu_X = \\mu_{X_1} + ... + \\mu_{X_n}$$\n",
    "\n",
    "If the random variables are independent, then we also have:\n",
    "\n",
    "\n",
    "$$\\sigma_X^2 = \\sigma_{X_1}^2 + ... + \\sigma_{X_n}^2$$\n",
    "\n",
    "__Note:__ The expected value of a sum is the sum of the expected values.\n",
    "\n",
    "\n",
    "####Defintion (Covariance)\n",
    "\n",
    "Let $X$ and $Y$ be random variables. The covariance of $X$ and $Y$, denoted by $\\sigma_{XY}$ or Cov$(X,Y)$ is:\n",
    "\n",
    "$$\\sigma_{XY} = \\mathbb{E}[X-\\mu_X)(Y - \\mu_Y)]$$\n",
    "\n",
    "Note that Cov$(X,Y)$ equals the variance of $X$. Also, if $X_1,...X_n$ are random variables and $X = X_1 + ... + X_n$ then:\n",
    "\n",
    "$$Var(X) = \\sum_{i=1}^n \\text{Var}(X_i) + 2\\sum_{1 \\le i \\lt j \\le n} \\text{Cov}(X_i, X_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.7 Skewness and Kurtosis\n",
    "\n",
    "Skewness and kurtosis are the third and fourth centered moments, respectively. These are the first moments at which you can see the shape of the distribution.\n",
    "\n",
    "Skewness measure the asymmetry of a distribution. If the skewness is negative, the tail on the left side is fatter or longer than the tail on the right side and vice versa if the skewness is postive. Symmetric distributions have skewness of zero.\n",
    "\n",
    "Kurtosis measure how the data peaks or flattens out, with respect to the normal distribution. Low kurtosis will have somewhat of a plateau at the mean. High kutosis will have a very sharp point at the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.8 Covariances\n",
    "\n",
    "Since independent variables have a covariance of 0, the covariance is a measure of how a change to one variable effects changes in another. Note however that a covariance of zeros does not imply that the two variables are independent. Covariance above zero shows that two variables are positively related, while a covariance below zero means they are inversely related.\n",
    "\n",
    "The __correlation__ is:\n",
    "\n",
    "$$\\rho = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "Correlation is a standardized version of the covariance, we always have $\\rho \\in [-1,1]$.\n",
    "\n",
    "$$\\text{Cov}(X,Y) = \\mathbb{E}[XY] - \\mu_X\\mu_Y$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
