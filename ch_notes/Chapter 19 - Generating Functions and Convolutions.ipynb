{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 19: Generating Functions and Convolutions\n",
    "\n",
    "##19.1 Motivation\n",
    "\n",
    "See A.12 for clever use of __multiplying by one__.\n",
    "\n",
    "###19.1.1 Definition: Sums of Poisson random variables\n",
    "\n",
    "The sum of $n$ independent Poisson random baraibles with parameters $\\lambda_1,...,\\lambda_n$ is a Poisson random variable with parameter $\\lambda_1 + ... + \\lambda_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##19.2 Definition\n",
    "\n",
    "####19.2.1 Def: Generating Function\n",
    "\n",
    "Given a sequence $\\{a_n\\}^{\\infty}_{n=0}$, we define its generating function by:\n",
    "\n",
    "$$G_a(s) = \\sum^{\\infty}_{n=0} a_n s^n$$\n",
    "\n",
    "for all $s$ where the sum converges.\n",
    "\n",
    "We can apply generating functions to find a closed form solution for the nth Fibonacci number (Binet's Formula)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##19.3 Uniqueness and Convergence of Generating Functions\n",
    "\n",
    "Depending on the sequence, it's possible for the generating function to exist for all $s$, for only some $s$, or only $s=0$ (which isn't saying much).\n",
    "\n",
    "###19.3.1 Theorem: Uniqueness of generating functions of sequences\n",
    "Let $\\{a_n\\}^{\\infty}_{n=0}$ and $\\{b_n\\}^{\\infty}_{n=0}$ be two sequences of numbers with generating functions $G_a(s)$ and $G_b(S)$ which converge for $|s| < \\delta$. Then the two sequences are equal if and only if $G_a(s) = G_b(s)$ for all $|s| < \\delta$. We may recover the sequence from the generating function by differentiating:\n",
    "\n",
    "$$a_n = \\frac{1}{n!} \\frac{d^n G_a(s)}{d s^n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##19.4 Convolutions I: Discrete Random Variables\n",
    "\n",
    "###19.4.1 Def: convolution of sequences\n",
    "\n",
    "If we have two sequences $\\{a_m\\}^{\\infty}_{m=0}$ and $\\{b_n\\}^{\\infty}_{n=0}$, we define their convolution to be the new sequence $\\{c_k\\}^{\\infty}_{k=0}$ given by:\n",
    "\n",
    "$$c_k = a_0b_k + a_1b_{k-1} + ... + a_{k-1}b_1 + a_k b_0 = \\sum_{l=0}^k a_l b_{k-l}$$\n",
    "\n",
    "We frequently write this as $c = a * b$.\n",
    "\n",
    "###19.4.2 Lemma\n",
    "Let $G_a(s)$ be the generating functions for $\\{a_n\\}^{\\infty}_{n=0}$ and $G_b(s)$ is the generating function for $\\{b_n\\}^{\\infty}_{n=0}$. Then the generating function of $c = a * b$ is $G_c(s) = G_a(s)G_b(s)$.\n",
    "\n",
    "###19.4.3 Def: Probability generating function\n",
    "Let $X$ be a discrete random variable taking on values in the integers. Let $G_X(s)$ be the generating function to $\\{a_m\\}^{\\infty}_{m=-\\infty}$ with $a_m = \\text{Prob}(X = m)$. Then $G_X(s)$ is called the _probability generating function._ If $X$ is only non-zero at the integers, then:\n",
    "\n",
    "$$G_X(s) = \\mathbb{E}[s^X] = \\sum_{m=-\\infty}^{\\infty} s^m \\text{Prob}(X = m)$$\n",
    "\n",
    "More generally, if the probabilities are non-zero on an at most countable set $\\{x_m\\}$ then:\n",
    "\n",
    "$$G_X(s) = \\mathbb{E}[s^X] = \\sum_{m}^{\\infty} s^{x_m} \\text{Prob}(X = x_m)$$\n",
    "\n",
    "###19.4.4 Theorem\n",
    "Let $X_1, ..., X_n$ be independent discrete random variables taking on non-negative integer values, with corresponding probability generating functions $G_{X_1}(s), ..., G_{X_n}(s)$. Then:\n",
    "\n",
    "$$G_{X_1 + ... + X_n} (s) = G_{X_1}(s) \\cdot .... \\cdot G_{X_n}(s)$$\n",
    "\n",
    "__The density of the sum of independent discrete random variables is the convolution of their probabilities.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##19.5 Convolutions II: Continuous random variables\n",
    "\n",
    "###19.5.1 Def: Pobability Generating Function\n",
    "\n",
    "Let $X$ be a continuous random variable with density $f$. Then:\n",
    "\n",
    "$$G_X(s) = \\int_{-\\infty}^{\\infty} s^x f(x) dx$$\n",
    "\n",
    "is the probability generating function of $X$.\n",
    "\n",
    "\n",
    "###19.5.2 Def: Convolution of Functions\n",
    "The convolution of two functions $f_1$ and $f_2$, denoted $f_1 * f_2$ is:\n",
    "\n",
    "$$(f_1 * f_2)(x) = \\int_{-\\infty}^{\\infty} f_1(t) f_2(x-t)dt$$\n",
    "\n",
    "If the $f_i$'s are densities then the integral converges.\n",
    "\n",
    "Note that this is a natural generalization of the convolution of two sequences, $c_k = \\sum a_l b_{k-l}$. In the discrete case, the two indices summed to the new index $k = l + k -l)$ while in the continuous case the sum of teh two arguments is the new argument $x = t + (x - t)$.\n",
    "\n",
    "\n",
    "###19.5.3 Theorem: Sums of continous random variables\n",
    "The probability density function of the sum of independent continuous random variables is the convolution of their probability density functions. In particular if $X_1,...,X_n$ have densities $f_1,...,f_n$ then the density of $X_1 + ... + X_n$ is $f_1 * f_2 * ... * f_n$.\n",
    "\n",
    "Proof involves changing the order of integration. This is always permissible in probability theory as our densities take on non-negative values, and thus Fubini's thereom holds (see B.2.1 for interchanging integrations).\n",
    "\n",
    "###19.5.4 Theorem: Commutativity of convolution\n",
    "The convolution of two sequences or functions is __commutative__; in other words, $a * b = b * a$ or $f_1 * f_2 = f_2 * f1$.\n",
    "\n",
    "Note: the sume of two exponential random variables is not exponential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##19.6 Definition and properties of moment generating functions\n",
    "\n",
    "###19.6.1 Def: Moments\n",
    "Let $X$ be a random variable with density $f$. Its $k^{th}$ __moment__, denoted $\\mu_k'$ is defined by:\n",
    "\n",
    "$$\\mu_k' = \\sum_{m=0}^{\\infty} x_m^k f(x_m)$$\n",
    "\n",
    "if $X$ is discrete, taking non-zero values only at the $x_m$'s and:\n",
    "\n",
    "$$\\mu_k' = \\int_{\\infty}^{\\infty} x^k f(x)dx$$\n",
    "\n",
    "if $X$ is continuous. In both cases we denote this as $\\mu_k' = \\mathbb{E}[X^k]$. We define the kth __centered moment__, $\\mu_k$ by $\\mu_k = \\mathbb{E}[(X - \\mu_1')^k]$. We frequently write $\\mu$ for the first moment and $\\sigma^2$ for the second centered moment.\n",
    "\n",
    "###19.6.2 Def: Moment generating function\n",
    "Let $X$ be a random variable with density $f$. The moment generating function of $X$, denoted $M_X(t)$, is given by $M_X(t) = \\mathbb{E}[e^{tX}]$. Explicitly, if $X$ is discrete then:\n",
    "\n",
    "$$M_X(t) = \\sum_{m= -\\infty}^{\\infty} e^{tx_m} f(x_m)$$\n",
    "\n",
    "While if $X$ is continuous then:\n",
    "\n",
    "$$M_X(t) = \\int_{-\\infty}^{\\infty} e^{tx} f(x) dx$$\n",
    "\n",
    "__Note:__ $M_x(t) = G_X(e^t)$ or $G_X(s) = M_X(\\log s)$.\n",
    "\n",
    "###19.6.3 Theorem: Properties of Moment Generating Functions\n",
    "1. We have:\n",
    "$$M_X(t) = 1 + \\mu_1't + \\frac{\\mu_2't^2}{2!} + \\frac{\\mu_3't^3}{3!} + ...$$\n",
    "In particular, $\\mu_k' = d^k M_X(t) / dt^k \\rvert_{t=0}$.\n",
    "\n",
    "2. Let $\\alpha$ and $\\beta$ be constants. Then $M_{\\alpha}X + \\beta(t) = e^{\\beta t} M_X(\\alpha t).$\n",
    "\n",
    "    Useful special cases are:\n",
    "     - $M_{X + \\beta} (t) = e^{\\beta t} M_X(t)$\n",
    "     - $M_{\\alpha X}(t) = M_X(\\alpha t)$\n",
    "     - (CLT) $M_{(x+\\beta)/\\alpha}(t) = e^{\\beta t / \\alpha} M_X(t/ \\alpha)$\n",
    "3. Let $X_1$ and $X_2$ be independent random variables with moment generating functions $M_{X_1} (t)$ and $M_{X_2} (t)$ with converge for $|t| < \\delta$. Then:\n",
    "\n",
    "$$M_{X_1 + X_2} (t) = M_{X_1}(t) M_{X_2} (t)$$\n",
    "\n",
    "This holds more generally for any sum of moment generating functions. If the random variables all have the same moment generating functions $M_X(t)$, then the right hand side becomes $M_X(t)^N$.\n",
    "\n",
    "\n",
    "###19.6.5 Theorem: Uniqueness of moment generating functions for discrete random variables.\n",
    "\n",
    "Let $X$ and $Y$ be discrete random variables taking on non-negative integer values with moment generating functions $M_X(t)$ and $M_Y(t)$, wach of whihc converges for $|t| < \\delta$. Then $X$ and $Y$ have the same distribution if and only if there is an $r > 0$ such that $M_X(t) = M_Y(t)$ for all $|t| < r$.\n",
    "\n",
    "__Discrete random variables are uniquely determined by their moment generating functions if they converge.__\n",
    "\n",
    "Sadly, this isn't always the case for continuous random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##19.7 Applications of Moment Generating Functions\n",
    "\n",
    "If we can compute the moment generating function of arandom variable, then we can find any moment simply by taking derivatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
